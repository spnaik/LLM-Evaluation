{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:47.123338Z","iopub.status.busy":"2024-03-06T13:31:47.122810Z","iopub.status.idle":"2024-03-06T13:32:06.224771Z","shell.execute_reply":"2024-03-06T13:32:06.223040Z","shell.execute_reply.started":"2024-03-06T13:31:47.123298Z"},"trusted":true},"outputs":[],"source":["!pip install -U langchain langchain_openai ragas sentence_transformers chromadb datasets lighthouz"]},{"cell_type":"markdown","metadata":{},"source":["## Import necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:21.132759Z","iopub.status.busy":"2024-03-06T13:29:21.132163Z","iopub.status.idle":"2024-03-06T13:29:34.884894Z","shell.execute_reply":"2024-03-06T13:29:34.883556Z","shell.execute_reply.started":"2024-03-06T13:29:21.132709Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/user/miniconda3/envs/lighthouse2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import pandas as pd\n","from datasets import load_dataset, Dataset\n","from langchain.document_loaders import WebBaseLoader\n","from dotenv import load_dotenv, find_dotenv\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain.chains import RetrievalQA\n","from langchain.chat_models import ChatOpenAI\n","from langchain.vectorstores import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.text_splitter import TokenTextSplitter\n","import requests\n","from langchain import LLMChain\n","import seaborn as sns\n","from bs4 import BeautifulSoup\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import QAGenerationChain\n","from langchain.output_parsers.json import parse_json_markdown\n","from langchain.llms import OpenAI\n","from langchain import PromptTemplate, LLMChain\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers import util\n","# from ragas.langchain.evalchain import RagasEvaluatorChain\n","\n","from ragas import evaluate\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    context_recall,\n","    context_precision,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### OpenAI api key"]},{"cell_type":"markdown","metadata":{},"source":["1. Run nano ~/.env\n","2. OPENAI_API_KEY = \"your open ai api key\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:34.887353Z","iopub.status.busy":"2024-03-06T13:29:34.886519Z","iopub.status.idle":"2024-03-06T13:29:34.893827Z","shell.execute_reply":"2024-03-06T13:29:34.892862Z","shell.execute_reply.started":"2024-03-06T13:29:34.887316Z"},"trusted":true},"outputs":[],"source":["# read local .env file\n","\n","_ = load_dotenv(find_dotenv()) "]},{"cell_type":"markdown","metadata":{},"source":["### Step 1: Generate chunks of data\n","\n","1.   Load the data science dojo page. \n","2.   Select a chunk size and chunk overlap size\n","3.   Split the entire page using TokenTextSplitter\n","4.   Use openai to create embeddings for each split\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:34.896993Z","iopub.status.busy":"2024-03-06T13:29:34.896308Z","iopub.status.idle":"2024-03-06T13:29:37.763148Z","shell.execute_reply":"2024-03-06T13:29:37.761553Z","shell.execute_reply.started":"2024-03-06T13:29:34.896958Z"},"trusted":true},"outputs":[],"source":["# load the Wikipedia page\n","loader = WebBaseLoader(\"https://datasciencedojo.com/blog/open-source-llms-for-enterprises-benefits/\")\n","\n","data = loader.load()\n","chunk_size = 4000\n","chunk_overlap = 200\n","text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","\n","#split the document as per chunksize and chunkoverlap\n","splitdocument = text_splitter.split_documents(data)\n","\n","#create embeddings\n","embeddings = OpenAIEmbeddings()"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2: Generate Questions and Ground truth answers\n","\n","The prompt below instructs the LLM to generate a question for each given chunk, and also generate an answer to the question. The answer is returned in a json format. "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:37.765349Z","iopub.status.busy":"2024-03-06T13:29:37.764921Z","iopub.status.idle":"2024-03-06T13:29:37.774193Z","shell.execute_reply":"2024-03-06T13:29:37.772732Z","shell.execute_reply.started":"2024-03-06T13:29:37.765315Z"},"trusted":true},"outputs":[],"source":["qa_template = \"\"\" Please generate a meaningful question whose answer is present in the given paragraph.\n","Also generate the answer to the question using the information in the given paragraph. Don't generate extra information not present in the given paragraph.\n","\n","\n","Example 1:\n","\n","Paragraph: The Moon is in geophysical terms a planetary-mass object or satellite planet. It has a mass that amounts to 1.2% of Earth's, and a diameter that is roughly one-quarter of Earth's or with 3,474 km (2,159 mi) about as wide as Australia.[17] Within the Solar System it is the most massive and largest satellite in relation to its parent planet, the fifth most massive and largest moon overall, and more massive and larger than all known dwarf planets.\n","Output:\n","{{\n","    \"Q\": \"How does the Moon's mass and size compare to Earth's?\",\n","    \"A\": \"The Moon, in geophysical context, is considered a planetary-mass object, possessing about 1.2% of the Earth's mass. Its diameter is roughly one-quarter that of Earth's, measuring approximately 3,474 kilometers, which is nearly equivalent to the width of Australia. \"\n","}}\n","\n","Example 2:\n","\n","Paragraph: Mars is the fourth planet from the Sun. The surface of Mars is orange-red because it is covered in iron(III) oxide dust, giving it the nickname \"the Red Planet\".[21][22] Mars is among the brightest objects in Earth's sky and its high-contrast albedo features have made it a common subject for telescope viewing. It is classified as a terrestrial planet and is the second smallest of the Solar System's planets with a diameter of 6,779 km (4,212 mi). In terms of orbital motion, a Martian solar day (sol) is equal to 24.5 hours and a Martian solar year is equal to 1.88 Earth years (687 Earth days). Mars has two natural satellites that are small and irregular in shape: Phobos and Deimos.\n","Output:\n","{{\n","    \"Q\": \"What are the two natural satellites of Mars? ,\n","    \"A\": \"The two natural satellites of Mars are Phobos and Deimos.\"\n","}}\n","\n","## PARAGRAPH:\n","{paragraph}\n","\n","\n","## OUTPUT FORMAT:\n","{{\n","    \"Q\": \"$Question\",\n","    \"A\": \"$Answer\"\n","}}\n","\n","\"\"\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:37.776831Z","iopub.status.busy":"2024-03-06T13:29:37.776332Z","iopub.status.idle":"2024-03-06T13:29:37.935851Z","shell.execute_reply":"2024-03-06T13:29:37.934724Z","shell.execute_reply.started":"2024-03-06T13:29:37.776788Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/user/miniconda3/envs/lighthouse2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n","  warn_deprecated(\n"]}],"source":["## chain for generating the QA\n","qa_prompt = PromptTemplate(template=qa_template, input_variables=[\"paragraph\"])\n","llm_chain = LLMChain(prompt=qa_prompt, llm = ChatOpenAI(temperature=0, model_name='gpt-4-turbo-preview'))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:37.938301Z","iopub.status.busy":"2024-03-06T13:29:37.937927Z","iopub.status.idle":"2024-03-06T13:29:37.946735Z","shell.execute_reply":"2024-03-06T13:29:37.945471Z","shell.execute_reply.started":"2024-03-06T13:29:37.938268Z"},"trusted":true},"outputs":[],"source":["## Let's take the top 2 splits only\n","def generate_qa(splitdocument):\n","    qa_generated = {'chunk': [], 'questions': [], 'ground_truths': []}\n","    for i in range(2):\n","        output = llm_chain.run(paragraph= splitdocument[i].page_content)   \n","        parsed_output = parse_json_markdown(output.strip())\n","        qa_generated['chunk'].append(i)\n","        qa_generated['questions'].append(parsed_output[\"Q\"])\n","        qa_generated['ground_truths'].append(parsed_output[\"A\"])\n","    return qa_generated"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:37.949109Z","iopub.status.busy":"2024-03-06T13:29:37.948563Z","iopub.status.idle":"2024-03-06T13:29:50.410592Z","shell.execute_reply":"2024-03-06T13:29:50.408903Z","shell.execute_reply.started":"2024-03-06T13:29:37.949075Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/user/miniconda3/envs/lighthouse2/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]}],"source":["get_qa_generated = generate_qa(splitdocument)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:50.413398Z","iopub.status.busy":"2024-03-06T13:29:50.412688Z","iopub.status.idle":"2024-03-06T13:29:50.421431Z","shell.execute_reply":"2024-03-06T13:29:50.420512Z","shell.execute_reply.started":"2024-03-06T13:29:50.413364Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame(get_qa_generated)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:50.426020Z","iopub.status.busy":"2024-03-06T13:29:50.425422Z","iopub.status.idle":"2024-03-06T13:29:50.791168Z","shell.execute_reply":"2024-03-06T13:29:50.790072Z","shell.execute_reply.started":"2024-03-06T13:29:50.425989Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>questions</th>\n","      <th>ground_truths</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>What are the benefits and challenges of using ...</td>\n","      <td>The benefits of using open-source LLMs include...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>How many individuals follow the LinkedIn newsl...</td>\n","      <td>Over 95,000 individuals trust the LinkedIn new...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   chunk                                          questions  \\\n","0      0  What are the benefits and challenges of using ...   \n","1      1  How many individuals follow the LinkedIn newsl...   \n","\n","                                       ground_truths  \n","0  The benefits of using open-source LLMs include...  \n","1  Over 95,000 individuals trust the LinkedIn new...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3: Build a RAG Application"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:29:50.793028Z","iopub.status.busy":"2024-03-06T13:29:50.792682Z","iopub.status.idle":"2024-03-06T13:30:56.235885Z","shell.execute_reply":"2024-03-06T13:30:56.234571Z","shell.execute_reply.started":"2024-03-06T13:29:50.792999Z"},"trusted":true},"outputs":[],"source":["collection_name = \"openai_collection\"\n","local_directory = \"openai_vect_embedding\"\n","persist_directory = os.path.join(os.getcwd(), local_directory)\n","vectorstore = Chroma.from_documents(documents=splitdocument, embedding=OpenAIEmbeddings(),collection_name=collection_name,\n","                      persist_directory=persist_directory)\n","vectorstore.persist()\n","retriever = vectorstore.as_retriever(return_source_document=True)\n","qa = RetrievalQA.from_chain_type(llm= ChatOpenAI(temperature=0, model='gpt-4-turbo-preview'), chain_type=\"stuff\", retriever=retriever,return_source_documents=True)\n","for i in range(len(df)):   \n","    retrieved_answers = qa({\"query\": df['questions'].iloc[i]})  \n","    df.loc[i, 'answers'] = retrieved_answers['result']\n","    df.loc[i,'contexts'] = retrieved_answers['source_documents'][0].page_content\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:30:56.238340Z","iopub.status.busy":"2024-03-06T13:30:56.237836Z","iopub.status.idle":"2024-03-06T13:30:56.255513Z","shell.execute_reply":"2024-03-06T13:30:56.253581Z","shell.execute_reply.started":"2024-03-06T13:30:56.238293Z"},"trusted":true},"outputs":[],"source":["\n","df"]},{"cell_type":"markdown","metadata":{},"source":["### Step 4: Evaluate using similarity metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:30:56.261285Z","iopub.status.busy":"2024-03-06T13:30:56.257445Z","iopub.status.idle":"2024-03-06T13:30:56.268840Z","shell.execute_reply":"2024-03-06T13:30:56.267947Z","shell.execute_reply.started":"2024-03-06T13:30:56.261244Z"},"trusted":true},"outputs":[],"source":["def similarity_eval(str1, str2):\n","    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","    embeddings1 = model.encode(str1, convert_to_tensor=True)\n","    embeddings2 = model.encode(str2, convert_to_tensor=True)\n","    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n","    \n","    return cosine_scores[0][0].item()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:30:56.270671Z","iopub.status.busy":"2024-03-06T13:30:56.270122Z","iopub.status.idle":"2024-03-06T13:31:04.412285Z","shell.execute_reply":"2024-03-06T13:31:04.410807Z","shell.execute_reply.started":"2024-03-06T13:30:56.270633Z"},"trusted":true},"outputs":[],"source":["for i in range(len(df)):   \n","    score = similarity_eval(df.loc[i, 'ground_truths'], df.loc[i, 'answers'] )\n","    df.loc[i, 'similarity_score'] = score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:04.415267Z","iopub.status.busy":"2024-03-06T13:31:04.414186Z","iopub.status.idle":"2024-03-06T13:31:04.434295Z","shell.execute_reply":"2024-03-06T13:31:04.432612Z","shell.execute_reply.started":"2024-03-06T13:31:04.415225Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["### Step 5: Evaluate using an LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:04.437354Z","iopub.status.busy":"2024-03-06T13:31:04.436686Z","iopub.status.idle":"2024-03-06T13:31:04.467633Z","shell.execute_reply":"2024-03-06T13:31:04.465833Z","shell.execute_reply.started":"2024-03-06T13:31:04.437319Z"},"trusted":true},"outputs":[],"source":["evaluation_prompt = \"\"\" You are an evaluator. You are given a question, a correct answer, and a given answer. \n","Your goal is to compare the correct answer and the given answer to output one of the two labels: `Correct` or `Incorrect`. \n","\n","Use the following definitions to output the labels:\n","** Correct: The given answer matches the correct answer. \n","\n","** Incorrect: The given answer does not match the correct answer. \n","\n","You are given the following information. \n","\n","Question: {question}\n","\n","Correct answer: {correct_answer}\n","\n","Given answer: {given_answer}\n","\n","Output only one word either `Correct` or `Incorrect`. You must provide an output. \n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:04.469620Z","iopub.status.busy":"2024-03-06T13:31:04.469132Z","iopub.status.idle":"2024-03-06T13:31:04.521547Z","shell.execute_reply":"2024-03-06T13:31:04.520330Z","shell.execute_reply.started":"2024-03-06T13:31:04.469579Z"},"trusted":true},"outputs":[],"source":["eval_llm = ChatOpenAI(\n","            model_name=\"gpt-4-turbo-preview\",\n","            temperature=0,\n","            request_timeout=120,\n","            openai_api_key=OPENAI_API_KEY\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:04.523833Z","iopub.status.busy":"2024-03-06T13:31:04.523025Z","iopub.status.idle":"2024-03-06T13:31:04.532198Z","shell.execute_reply":"2024-03-06T13:31:04.530583Z","shell.execute_reply.started":"2024-03-06T13:31:04.523800Z"},"trusted":true},"outputs":[],"source":["def run_llm_eval(question, correct_answer, given_answer):\n","    eval_prompt = PromptTemplate(\n","            template=evaluation_prompt, input_variables=[\"question\", \"correct_answer\", \"given_answer\"]\n","        )\n","    \n","    llm_chain = LLMChain(prompt=eval_prompt, llm=eval_llm)\n","    output = llm_chain.run(question=question, correct_answer=correct_answer, given_answer=given_answer)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:04.534711Z","iopub.status.busy":"2024-03-06T13:31:04.534018Z","iopub.status.idle":"2024-03-06T13:31:05.929885Z","shell.execute_reply":"2024-03-06T13:31:05.928304Z","shell.execute_reply.started":"2024-03-06T13:31:04.534671Z"},"trusted":true},"outputs":[],"source":["for i in range(len(df)):   \n","    output = run_llm_eval(question=df.loc[i, 'questions'], \n","                         correct_answer=df.loc[i, 'ground_truths'], \n","                         given_answer=df.loc[i, 'answers'])\n","    df.loc[i, 'llm_eval_score'] = output == 'Correct'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:05.931961Z","iopub.status.busy":"2024-03-06T13:31:05.931361Z","iopub.status.idle":"2024-03-06T13:31:05.944992Z","shell.execute_reply":"2024-03-06T13:31:05.944180Z","shell.execute_reply.started":"2024-03-06T13:31:05.931927Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["### Step 6: Run eval with RAGAS "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:05.946807Z","iopub.status.busy":"2024-03-06T13:31:05.946271Z","iopub.status.idle":"2024-03-06T13:31:21.652882Z","shell.execute_reply":"2024-03-06T13:31:21.651617Z","shell.execute_reply.started":"2024-03-06T13:31:05.946777Z"},"trusted":true},"outputs":[],"source":["from ragas.metrics import (\n","    answer_relevancy,\n","    faithfulness,\n","    context_recall,\n","    context_precision,\n",")\n","from ragas import evaluate\n","\n","data = {\n","    \"question\": df[\"questions\"].tolist(),\n","    \"answer\": df[\"answers\"].tolist(),\n","    \"contexts\": [[i] for i in df[\"contexts\"].tolist()],\n","    \"ground_truth\": df[\"ground_truths\"].tolist()\n","}\n","dataset = Dataset.from_dict(data)\n","\n","result = evaluate(\n","    dataset,\n","    metrics=[\n","        context_precision,\n","        faithfulness,\n","        answer_relevancy,\n","        context_recall,\n","    ],\n",").to_pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:31:21.654779Z","iopub.status.busy":"2024-03-06T13:31:21.654418Z","iopub.status.idle":"2024-03-06T13:31:21.671024Z","shell.execute_reply":"2024-03-06T13:31:21.669903Z","shell.execute_reply.started":"2024-03-06T13:31:21.654749Z"},"trusted":true},"outputs":[],"source":["result"]},{"cell_type":"markdown","metadata":{},"source":["### Step 7: Run eval with Lighthouz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:32:21.077826Z","iopub.status.busy":"2024-03-06T13:32:21.077252Z","iopub.status.idle":"2024-03-06T13:32:21.125023Z","shell.execute_reply":"2024-03-06T13:32:21.123464Z","shell.execute_reply.started":"2024-03-06T13:32:21.077784Z"},"trusted":true},"outputs":[],"source":["import os\n","import requests\n","\n","from lighthouz import Lighthouz\n","from lighthouz.benchmark import Benchmark\n","from lighthouz.app import App\n","from lighthouz.evaluation import Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:32:21.127616Z","iopub.status.busy":"2024-03-06T13:32:21.127184Z","iopub.status.idle":"2024-03-06T13:32:21.134372Z","shell.execute_reply":"2024-03-06T13:32:21.133119Z","shell.execute_reply.started":"2024-03-06T13:32:21.127582Z"},"trusted":true},"outputs":[],"source":["LH = Lighthouz(\"LIGHTHOUZ-API-KEY\") # Add your Lighthouz API key. To obtain a Lighthouz API key contact srijan@lighthouz.ai\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:33:49.070583Z","iopub.status.busy":"2024-03-06T13:33:49.070072Z","iopub.status.idle":"2024-03-06T13:33:51.850841Z","shell.execute_reply":"2024-03-06T13:33:51.848653Z","shell.execute_reply.started":"2024-03-06T13:33:49.070548Z"},"trusted":true},"outputs":[],"source":["!mkdir EXAMPLE-DATA\n","!wget https://d18rn0p25nwr6d.cloudfront.net/CIK-0000320193/b4266e40-1de6-4a34-9dfb-8632b8bd57e0.pdf -O ./EXAMPLE-DATA/apple-10K-2022.pdf\n","RAG_DOCUMENT = \"./EXAMPLE-DATA/apple-10K-2022.pdf\"  # you can provide any pdf file or folder with pdf files to create the RAG benchmark\n","RAG_DIRECTORY = \"./EXAMPLE-DATA/\""]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Generate a RAG benchmark with Lighthouz AutoBench"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:33:51.853867Z","iopub.status.busy":"2024-03-06T13:33:51.853451Z","iopub.status.idle":"2024-03-06T13:33:51.861107Z","shell.execute_reply":"2024-03-06T13:33:51.859216Z","shell.execute_reply.started":"2024-03-06T13:33:51.853830Z"},"trusted":true},"outputs":[],"source":["# Benchmark id is available on the lighthouz dashboard.\n","benchmark_id = \"659b66198e4cc1f4af4e2373\" # this is the pre-loaded finance benchmark on apple's 10-K report."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:33:51.863123Z","iopub.status.busy":"2024-03-06T13:33:51.862800Z","iopub.status.idle":"2024-03-06T13:33:51.872843Z","shell.execute_reply":"2024-03-06T13:33:51.871756Z","shell.execute_reply.started":"2024-03-06T13:33:51.863095Z"},"trusted":true},"outputs":[],"source":["# benchmark_categories = [\"rag_benchmark\"]\n","# benchmark_generator = Benchmark(LH)\n","# benchmark_data = benchmark_generator.generate_benchmark(file_path=RAG_DOCUMENT, benchmark_categories=benchmark_categories)\n","# benchmark_id = benchmark_data[\"benchmark_id\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Connect your RAG app on Lighthouz  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:33:51.875849Z","iopub.status.busy":"2024-03-06T13:33:51.875426Z","iopub.status.idle":"2024-03-06T13:34:02.692678Z","shell.execute_reply":"2024-03-06T13:34:02.690889Z","shell.execute_reply.started":"2024-03-06T13:33:51.875815Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","from langchain.chains import RetrievalQA\n","from langchain.chat_models import ChatOpenAI\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.text_splitter import TokenTextSplitter\n","from langchain.vectorstores.chroma import Chroma\n","from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n","from langchain import HuggingFacePipeline\n","from langchain.prompts import PromptTemplate\n","\n","def langchain_rag_model(llm=\"gpt-3.5-turbo\"):\n","    \"\"\"\n","    This is a RAG model built with Langchain, OpenAI, and Chroma\n","    \"\"\"\n","    print(\"Initializing LangChain RAG OpenAI Agent\")\n","\n","    chunk_size = 2000\n","    chunk_overlap = 150\n","    collection_name = \"data-test_vect_embedding\"\n","    local_directory = \"data-test_vect_embedding\"\n","    persist_directory = os.path.join(os.getcwd(), local_directory)\n","    if not os.path.exists(persist_directory) or not os.listdir(persist_directory):\n","        embeddings = OpenAIEmbeddings()\n","        documents = []\n","        if RAG_DOCUMENT.endswith(\".pdf\"):\n","            loader = PyPDFLoader(RAG_DOCUMENT)\n","            documents.extend(loader.load())\n","        else:\n","            for file in os.listdir(RAG_DOCUMENT):\n","                if file.endswith(\".pdf\"):\n","                    pdf_path = os.path.join(RAG_DOCUMENT, file)\n","                    loader = PyPDFLoader(pdf_path)\n","                    documents.extend(loader.load())\n","        text_splitter = TokenTextSplitter(\n","            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n","        )\n","        splitdocument = text_splitter.split_documents(documents)\n","        vectDB = Chroma.from_documents(\n","            splitdocument,\n","            embeddings,\n","            collection_name=collection_name,\n","            persist_directory=persist_directory,\n","        )\n","        vectDB.persist()\n","    else:\n","        # Load the existing vector store\n","        embeddings = OpenAIEmbeddings()\n","        vectDB = Chroma(\n","            collection_name=collection_name, persist_directory=persist_directory, embedding_function=embeddings\n","        )\n","\n","    # LLM used in RAG\n","    if llm ==\"gpt-3.5-turbo\":\n","        llm_model = ChatOpenAI(\n","        model_name=\"gpt-3.5-turbo\",\n","        temperature=0,\n","        request_timeout=120,\n","        )\n","    elif llm == \"gpt-4\":\n","        llm_model = ChatOpenAI(\n","            model_name=\"gpt-4\",\n","            temperature=0,\n","            request_timeout=120,\n","            )\n","\n","    retriever = vectDB.as_retriever(return_source_document=True)\n","\n","    # prepare stuff prompt template\n","    prompt_template = \"\"\"You are a helpful assistant. Your job is to provide the answer for the question based on the given context.\n","    ## CONTEXT: {context}\n","    ## QUESTION: {question}\n","    ## ANSWER: \"\"\".strip()\n","\n","    prompt = PromptTemplate(\n","        input_variables=[\"context\", \"question\"],\n","        template=prompt_template\n","    )\n","\n","    rag_model = RetrievalQA.from_chain_type(\n","        llm=llm_model,\n","        retriever=retriever,\n","        chain_type=\"stuff\",\n","        return_source_documents=False,\n","        chain_type_kwargs={\"prompt\":prompt}\n","    )\n","    print(\"Langchain RAG OpenAI agent has been initialized.\")\n","    return rag_model\n","\n","rag_model = langchain_rag_model(llm=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:34:02.696121Z","iopub.status.busy":"2024-03-06T13:34:02.695720Z","iopub.status.idle":"2024-03-06T13:34:02.702534Z","shell.execute_reply":"2024-03-06T13:34:02.701163Z","shell.execute_reply.started":"2024-03-06T13:34:02.696090Z"},"trusted":true},"outputs":[],"source":["def langchain_rag_query_function(query: str) -> str:\n","    \"\"\"\n","    This is a function to send queries to the RAG model\n","    \"\"\"\n","    response = rag_model({\"query\": query})[\"result\"]\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:34:02.704855Z","iopub.status.busy":"2024-03-06T13:34:02.704362Z","iopub.status.idle":"2024-03-06T13:34:02.985033Z","shell.execute_reply":"2024-03-06T13:34:02.983477Z","shell.execute_reply.started":"2024-03-06T13:34:02.704815Z"},"trusted":true},"outputs":[],"source":["app = App(LH)\n","app_data = app.register(name=\"gpt-3.5-turbo\", model=\"gpt-3.5-turbo\")\n","app_id = app_data[\"app_id\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Evaluate the RAG app on the benchmark with Lighthouz AutoEval"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:34:02.987992Z","iopub.status.busy":"2024-03-06T13:34:02.986911Z","iopub.status.idle":"2024-03-06T13:34:30.568287Z","shell.execute_reply":"2024-03-06T13:34:30.566675Z","shell.execute_reply.started":"2024-03-06T13:34:02.987940Z"},"trusted":true},"outputs":[],"source":["evaluation = Evaluation(LH)\n","e_single = evaluation.evaluate_rag_model(\n","    response_function=langchain_rag_query_function,\n","    benchmark_id=benchmark_id,\n","    app_id=app_id,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Compare multiple RAG apps on the benchmark with Lighthouz Arena"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:34:41.166083Z","iopub.status.busy":"2024-03-06T13:34:41.165674Z","iopub.status.idle":"2024-03-06T13:34:41.461242Z","shell.execute_reply":"2024-03-06T13:34:41.459726Z","shell.execute_reply.started":"2024-03-06T13:34:41.166052Z"},"trusted":true},"outputs":[],"source":["rag_model_gpt4 = langchain_rag_model(llm=\"gpt-4\")\n","\n","def langchain_rag_query_function_gpt4(query: str) -> str:\n","    \"\"\"\n","    This is a function to ask queries to the RAG model with GPT4\n","    \"\"\"\n","    response = rag_model_gpt4({\"query\": query})[\"result\"]\n","    return response\n","\n","app = App(LH)\n","app_data = app.register(name=\"gpt-4\", model=\"gpt-4\")\n","app_id_gpt4 = app_data[\"app_id\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T13:34:41.463616Z","iopub.status.busy":"2024-03-06T13:34:41.463244Z","iopub.status.idle":"2024-03-06T13:34:57.407849Z","shell.execute_reply":"2024-03-06T13:34:57.406324Z","shell.execute_reply.started":"2024-03-06T13:34:41.463587Z"},"trusted":true},"outputs":[],"source":["e_multiple = evaluation.evaluate_multiple_rag_models(\n","    response_functions=[langchain_rag_query_function, langchain_rag_query_function_gpt4],\n","    benchmark_id=benchmark_id,\n","    app_ids=[app_id, app_id_gpt4],\n",")\n","print(e_multiple)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
